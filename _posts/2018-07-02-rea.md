---
layout: post
title: The Resonance of Emotion in Audio
---

<img src="/assets/images/rea/emotion_masks.jpg">

We live in a world of ever-increasing interaction between humans and AI - examples include Siri, Alexa and Google Assistant. These devices are quite capable of interpreting content - *what* is being communicated. But the user experience could be greatly enhanced if AI had the ability to identify the emotional context of the interaction - *how* it's being communicated.

## Summary
This project explored machine learning's ability to identify emotion in audio recordings of human speech.

### Data
The dataset for this project was based on the RAVDESS database which contains over 1,400 audio files in American English. Each file is spoken by one of twenty-four actors, portrays one of eight emotions and contains one of two sentences: "Kids are talking by the door" and "Dogs are sitting by the door".

### Method
To develop a model for the analysis, features needed to be extracted from the audio files. Librosa, a python library, was utilized for this task. First the time series is retrieved for each file. Then librosa extracts the Mel-Frequency Cepstral Coefficients through a Short-Time Fourier Transform. These coefficients approximate the human auditory response and are commonly used for speech recognition applications. These coefficients create a matrix of the intensity of the coefficients over time - it can be thought of as creating an image for each audio observation. Over 3,000 features were extracted for each observation.

<img src="/assets/images/rea/features.png">

### Results
Several classification models were considered and, after some tuning, a polynomial SVM classifier was selected. This model was able to accurately label emotions just under 60% of the time which is a significant improvement over a one out of eight random guess (12.5%). However when the model's top three predicted emotions are considered, accuracy increases to 87%.

<img src="/assets/images/rea/acc.png">


Model accuracy was mostly consistent across emotional classes. One notable exception is that Happy has a tendency to be misclassified as Surprised.

<img src="/assets/images/rea/conf.png">

### Conclusions
This model demonstrates that there are properties of emotion that resonate in audio recordings of human speech and they can be identified through machine learning. A major challenge throughout the project was the limitations of the dataset used. The model was overfit as a result.

Model performance could be improved through the use of unscripted, natural observations of conversation. These observations would provide more realistic training data and include a greater diversity of speakers.
